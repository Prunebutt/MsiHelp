% -----------------------------------------------------------------
%		 				 Dynamic Models
% -----------------------------------------------------------------

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=\textbf{Continuous Time Systems}]
	\textbf{Ordinary Differential Equations (ODE):}
	\begin{align*}
	\dot{ x } &= f( x(t), u(t), \epsilon(t), p)
	\end{align*}
	
	\textbf{Differential Algebraic Equations(DAE):}
	\begin{align*}
	\dot{ x } &= f( x(t), u(t), \epsilon(t), p)\\
	0 &= g(x, z).
	\end{align*}
	
	\textbf{LTI Sytem (ODE):}
	\begin{align*}
	\dot x &= Ax+Bu \quad y = Cx+Du \\
	G(s) &= C (sI-A)^{-1} B+D
	\end{align*}
	\textbf{Zero order hold}\\
	Constant control $\ti{u}{const}$ for time steps of length $\Delta t$
	\begin{align*}
	  x(t;x_0\ti{u}{const})=e^{A\Delta t}\cdot x_0+\int^{\Delta t}_0e^{A(\Delta t-\tau)}B\ti{u}{const}\diff\tau
	\end{align*}
\end{tcolorbox}


\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=\textbf{Numerical Integration Methods}]
	
	\textbf{Euler Integration Step}
	\begin{align*}
	\tilde{x}(t; x_0, \ti{u}{const}) &= x_0 + t f(x_0, \ti{u}{const}), \quad t \in [0, \Delta t]\\
	\tilde{x}_{j+1} &= \tilde{x}_j + h f (\tilde{x}_j, \ti{u}{const}), \quad j = 0, ..., M - 1
	\end{align*}
	\begin{itemize}
		\item Approximation becomes better by decreasing the step size h.
		\item Concistency Error: $h^2$
		\item Total Number of steps: $\Delta t / h$
		\item Error in the final step of order $h \Delta t$
		\item Linear in step size $\rightarrow$ order one
		\item Taking more steps is more accurate but needs more computation
	\end{itemize}
	
	\textbf{Runge-Kutta Method of Order Four (RK4)}
	\begin{align*}
		k_1 &= f(\tilde{x}_j, \ti{u}{const})\\
		k_2 &= f(\tilde{x}_j, \frac{h}{2} k_1, \ti{u}{const})\\
		k_3 &= f(\tilde{x}_j, \frac{h}{2} k_2, \ti{u}{const})\\
		k_4 &= f(\tilde{x}_j, h k_3, \ti{u}{const})\\
		\tilde{x}_{j+1} &= \tilde{x}_j + \frac{h}{6} (k_1 + 2k_2 + 2k_3 + k_4)
	\end{align*}
	
	One Step of RK4 is thus as expensive as four steps of euler\\
	accurency of final approximation is of order $h^4 \Delta$ t\\
	$\rightarrow$ RK4 needs fewer functions to obtain the same accuracy level as euler
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=\textbf{Discrete Time Systems}]
	\begin{tabular}{ll}
		Det. Model as State Space  & Stoch. Model as State Space  \\
		Det. Model as Input-Output  & Stoch. Model as Input-Output 
	\end{tabular}

	\textbf{State Space Model}\\
	$ x_{k+1} = f_k(x_k,u_k), k = 0,1,\dots,N-1$ with input vector $u_k$ and state vector $x_k$
	
	\textbf{Input-Output Model}\\
	$y(k) = h(u(k),\dots,u(k-n),y(k-1),\dots,y(k-n))$
	\\
	
	\textbf{LTI system as State-Space Model:}
	\begin{align*}
	  x_{k+1} &= A x_k + B u_k, \quad k = 0, 1,..., N - 1.
	\end{align*}
	\textbf{LTI system as Input-Output Model:}
	\begin{flalign*}
	G(s) &= \frac{ b_0 + b_1s+...+b_ns^n }{ a_0+a_1s+...+a_{n-1}s^{n-1}+s^n } \quad | \cdot s = z^{-1} &\\
	G(z) &= \frac{ b_0 + b_1 z^{-1}+...+b_n z^{-n} }{a_0+a_1z^{-1}+...+a_n z^{-n}} &\\
	&= \frac{ b_0z^n+b_n z^{n-1}+...+b_n}{a_0 z^n+a_1 z^{n-1}+...+a_n} \quad \Rightarrow \text{Also called "polynomial model".} &
	\end{flalign*}	
\end{tcolorbox}		
\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=\textbf{Deterministic Model}]
  The output of the system can be obtained with absolute certainty. The Output $y$ or the state $x$, depend on the known inputs $u(1), \dots, u(N)$, the previous Outputs $y(1), \dots, y(N)$ or state $x(n-1)$ and initial conditions. \\
  All deterministic models are \textbf{time invariant}.\\
\textbf{State Space Model: }
\begin{align*}
	 x(k+1) &= f(x(k), u(k)) \\
	y(k) &= g(x(k), u(k)) 
\end{align*}
Initial conditions: $x(1) = x_0$

\textbf{Input-Output Model}
\begin{align*}
y(k) &= h(u(k), ..., u(k-n), y(k-1), ..., y(k-n))
\end{align*}
Initial conditions: $y(1) = y_1,\dots,y(n) = y_n  \;\; u(1)= u_1,\dots,u(n) = u_n$

\textbf{Finite Impulse Response (FIR): } 
\begin{align*}
	y(k) &= b_0 u(k) + ... + b_{n_b} u(k-n_b) \\
	G(z) &= b_0 + b_1z^{-1} + ... + b_{n_b}z^{-n_b} \quad | \cdot \frac{z^{n_b}}{z^{n_b}} \\
	&= \frac{b_0 z^{n_b} + b_1 z^{ n_{b-1} } + ... + b_{n_b} }{z^{n_b}}
\end{align*}

\textbf{Auto Regressive model with eXogenous inputs (ARX/IRR):}
\begin{align*}
	a_0y(k)+\dots+a_{n_a}y(k-n_a) = b_0u(k) + \dots+b_{n_b}u(k-n_b)
\end{align*}
\begin{align*}
G(z) = \frac{b_0z^n + b_1z^{n-1} + \cdots + b_n}{a_0z^n + a_1z^{n-1} + \cdots + a_n}
\end{align*}
The next output depends on the previous output. Also called \textbf{IIR} (infinite impulse response)\\

\textbf{Auto Regressive model(AR):}
\begin{align*}
	y(k)= a_1y(k-1)-...-a_{n_a}y(k-n_a)
\end{align*}
\end{tcolorbox}
\includegraphics[width=0.3\textwidth]{unit_circle}
\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=\textbf{Stochastic Model}]
	Real systems are far from deterministic. 
	\begin{itemize}
		\item there is stochastic noise $\varepsilon(k)$ 
		\item there are constant and unknown parameters $p$
		\item measured outputs $y(k)$ depend in both, $\varepsilon(k)$ and $p$ 
	\end{itemize}
	
	Assumptions: noise is \textbf{i.i.d} and enters the model like a normal input, but as a random variable\\
	
	\textbf{State Space Model}
	\begin{align*}
	x(k+1) = f(x(k), u(k), \varepsilon(k)) \\
	y(k) = g(x(k), u(k), \varepsilon(k))
\end{align*}

\textbf{Input-Output Model}\\
Only interested in input and output, not the whole model state
\begin{flalign*}
	y(k) &= h(u(k), ..., u(k-n), y(k-1), ..., y(k-n), \varepsilon(k), ..., \varepsilon(k-n)) &\\
	\quad \text{for} \quad k &= n + 1, n + 2, ... &
\end{flalign*}	

\textbf{Measurement Noise (Output Error Model)}
\begin{align*}
	y(k) = M(k; U, x_0, p) + \varepsilon(k)
\end{align*}
\tcblower
\textbf{Stochastic Disturbance (Equation Errors)}
\begin{align*}
	y(k) &= h(p, u(k), ..., u(k-n), y(k-1), ..., y(k-n)) + \varepsilon(k) \\
	&\text{for } k = n + 1, n + 2, \ldots
\end{align*}

\textbf{Linear In the Parameters models (LIP):}
\begin{align*}
	y(k) &= \sum_{ i = 1}^{d}\theta_i\phi_i(u(k), \dots ,y(k-1), \dots)+\varepsilon(k)\\
	y(k) &= \varphi(k)^T\theta + \varepsilon(k) \quad \text{where} \, \varphi = (\phi_1(\cdot),... ,\phi_d(\cdot)) 
\end{align*}

\textbf{LIP-LTI Models with Equation Errors (ARX)\footnote{additive noise is a special case}} 

\hspace{1em}- Combining best of two worlds (LTI and LIP)
\begin{flalign*}
	&\quad a_0y(k) + \ldots +a_{n_{a}}y(k-n_a) = b_0u(k) + \ldots + b_{n_{b}}u(k-n_b) + \varepsilon(k) &
\end{flalign*}

\textbf{Auto-Regressive Moving Average with eXogeneous input \\(ARMAX):}
\begin{flalign*}
	 a_0y(k) + ... + a_{n_a}y(k-n_a) &= b_0u(k) + ... + b_{n_b} u(k - n_b) + \varepsilon(k) + \\
	& \qquad c_1 \varepsilon(k-1) + ... + c_{n_x} \varepsilon(k-n_c) &
\end{flalign*}

\textbf{Auto-Regressive Moving Average without inputs (ARMA):}
\begin{flalign*}
	&\quad a_0y(k) + ... + a_{n_a}y(k-n_a) = \varepsilon(k) + c_1 \varepsilon(k-1) + ... + c_{n_x} \varepsilon(k-n_c)&
\end{flalign*}
Where $c_i$ represent the noise coefficient, we have to use non-linear least squares with the unknown noise terms $\varepsilon(k-i)$

\textbf{Difference between Deterministic and Stochastic Models}
\begin{itemize}
	\item stochastic noise $\varepsilon(k)$
	\item unknown but constant parameter $p$
	\item measured output $y(k)$ depend on both, $\varepsilon(k)$ and $p$
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=\textbf{Example for State Space Model}]
	$\ddot{a} = m \cdot \dot{a} + g\cdot a + c \cdot u$\\
	$y = \dot{a}$\\
	$x = \begin{bmatrix}
		a \\ \dot{a}
	\end{bmatrix}$
	$
	\dot{x} = \begin{bmatrix}
	\dot{a} \\ \ddot{a}
	\end{bmatrix}
	$
	$ \dot x = Ax+Bu \quad y = Cx+Du $
	\\
	$
	A = \begin{bmatrix}
	0 & 1 \\
	g & m \\
	\end{bmatrix}
	$
	$ B= \begin{bmatrix}
	0 \\
	c
	\end{bmatrix}$
	$C = \begin{bmatrix}
	0 & 1
	\end{bmatrix} $
	$D = \begin{bmatrix}
	0
	\end{bmatrix}$
	\todo[inline]{check it}
\end{tcolorbox}

\begin{tcolorbox}[colback=purple!5!white,colframe=purple!75!black,title=\textbf{Pure Output Error (OE) Minimization}]
Assume: i.i.d. gaussian noise only affecting output\\
using non-linear least squares
\begin{align*}
	\theta_{ML} =\underset{\theta}{\text{min}} \sum_{k=1}^{N} (y(k)-M(k;U, x_0 p))^2
\end{align*}

\textbf{Output Error Minimization for FIR Models:}
lead to convex problems, therefore global minimum can be found
\begin{flalign*}
	y(k) &= (u(k), u(k-1), ..., u(k-n_{n_b})) \cdot \theta +\varepsilon(k) &\\
	&= \underset {\theta}{ \text{min} } \sum_{k=n_{b}+1}^{N} (y(k)-\underbrace{(u(k), u(k-1),... , u(k-n_{n_b}))}_{\text{det. part is also}\, M(k; U, x_0, p)} \cdot \theta)^2 &
\end{flalign*}
They often need a very high dimension $n_b$ to obtain a reasonable fit. As a consequence ARX models are usually used instead. \\

\textbf{Equation Error Minimization:}
Assume: i.i.d. $\varepsilon(k)$ noise enters the input-output equation as additive disturbance
\begin{align*}
	y(k) &= h(p, u(k), ..., u(k-n), y(k-1), ..., y(k-n)) + \varepsilon(k)\\
	\text{for} \quad k &= n + 1, n + 2
\end{align*}

if the i.i.d noise is gaussian, a maximum likelihood formulation to estimate the unknown parameter vector $\theta = p$ is given:
\begin{align*}
	\theta_{ML} = \underset {\theta}{ \text{min} } \sum_{k = n + 1}^{N}{(y(k) - h(p, u(k), ..., y(k-1), ...)) )^2}
\end{align*}
u and k are known input and output measurements, and the algorithm minimises the so called \textbf{equation errors} or \textbf{prediction errors}.

This problem is also known as \textbf{Prediction error minimisation(PEM)}
Such a problem is convex if $p$ enters linearly in $f$, i.e. if the model is \textbf{linear-in-the-parameters (LIP)} \\

\textbf{PEM of LIP Models}
\begin{align*}
	y(k) &= \varphi(k)^T\theta + \varepsilon(k)\\
	\quad \text{where} \, \varphi &= (\phi_1(\cdot),... ,\phi_d(\cdot))^T \text{are the regressor variables}
\end{align*}

considering this last expression, the prediction error minimisation(PEM) problem can be formulated as:
\begin{align*}
	\underset{\theta}{\text{min}} \underbrace{\sum_{k=1}^{N} (y(k)-\varphi(k)^\text{T} \theta)^2}_{= \parallel y_N - \Phi_N \theta \parallel_2^2}
\end{align*}
Which can be solved using LLS $\theta^* = \Phi^+_N y_N$ \\

\textbf{Special Case: PEM of LIP-LTI Models with Equation Errors(ARX)}
General ARX model equation
\begin{align*}
	a_0y(k)+...+a_{n_{a}}y(k-n_a) = b_0u(k)+...+b_{n_{b}}u(k-n_b)+\varepsilon(k) 
\end{align*}

In order to have a determined estimation problem, $a_0$ has to be fixed, otherwise the number of optimal solutions would be infinitive. Therefore we sually fix $a_0 = 1$ and use $\theta = (a_1, ..., a_{n_a}, b_0, ..., b_{n_b})^\text{T}$ as the parameter estimation vector. The regressor vector is given by:
\begin{align*}
\varphi = (-y(k-1), ..., -y(k-n_a), u(k), ..., u(k-n_b))^\text{T}
\end{align*}
leading to the optimal solution provided by LLS:
\begin{align*}
y(k) = \varphi(k)^\text{T} \theta + \varepsilon(k)
\end{align*}
\end{tcolorbox}

\begin{tcolorbox}[colback=purple!5!white,colframe=purple!75!black,title=\textbf{Pure Output Error (OE) Minimization}]
\textbf{Models with Input and Output Errors:}
\begin{align*}
	y(k)=M(k;U + \varepsilon_{N}^{u}, x_0, p) + \varepsilon^y(k)
\end{align*}

\includegraphics[width=.75\textwidth]{model.pdf}

Assume: i.i.d. gaussian noise on both input and output with variance $\sigma_u^2$ for the input and $\sigma_y^2$ for the output
\begin{flalign*}
	\hat{\theta}=& \argmin_\theta \sum_{k-1}^{N} \frac{1}{\sigma_{y}^{2}} (y(k)-M(k;U+ \varepsilon_{N}^{u},x_0,p))^2 + \frac{1}{\sigma_{u}^{2}} (\varepsilon_u (k))^2 & \\
	\hat{\theta}=& \argmin_{\theta} \sum_{k-1}^{N} \frac{1}{\sigma_{y}^{2}} (y(k)-M(k;\tilde U, x_0, p))^2 + \frac{1}{\sigma_{u}^{2}} (u(k)-\tilde u(k) )^2 &
  \end{flalign*}
	\\
	\hspace{-2pt}
	$\tilde{U}:=U+\varepsilon^u_N$
\end{tcolorbox}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../HelpSheet"
%%% End:
