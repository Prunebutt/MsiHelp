% -----------------------------------------------------------------
% 				 Linear Least Squares Estimation
% -----------------------------------------------------------------
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=\textbf{Linear Least Squares Estimation}]
\textbf{Preliminaries:} i.i.d. and Gaussian noise
\begin{flalign*}
	& \textbf{Overall Model: }
	y(k)= \phi (k)^T \theta +\varepsilon (k) & \\
	& \textbf{LS cost function as sum: }
	\sum _{k=1}^{N} { (y(k)- \phi(k)^T \theta )}^{2} & \\
	& \textbf{LS cost function: }
	f(\theta )={ \lVert y_N - \Phi_N \theta \rVert }_{2}^{2} & \\
	&\textbf{Unique minimizers: }
	\hat{\theta}_{LS} = \underset{\theta \in \mathbb{R}}{arg\,min} \, f(\theta) \hfil \theta^* = \underbrace{(\Phi^T \Phi)^{-1} \Phi^T}_{:=\Phi^+} y & \\
	& \textbf{Pseudo Inverse: } \Phi ^+ = (\Phi^T \Phi)^{-1} \Phi^T &
\end{flalign*}
\end{tcolorbox}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=\textbf{Weighted Least Squares (unitless)}]
\textbf{For i.i.d noise:} Unweight Least Squares is optimal: $W = I$
\begin{align*}
  f_{WLS}(\theta) &= \sum _{ k=1 }^{ N }\frac {{{ (y(k)-{ \phi (k) }^{ T }\theta )}^{2  } }}{\sigma_{\epsilon}^{2}(k)} = { \lVert y_N - \Phi_N \theta \rVert }_{W}^{2} \\
				  &= \lVert \matrixroot{W}y - \matrixroot{W}\Phi_N\theta\rVert^2_2 = { (y_N - \Phi \cdot \theta ) }^{T} \cdot W \cdot (y_N - \Phi \cdot \theta )
\end{align*}
\textbf{Solution for WLS: }
\begin{flalign*}
	\hat \theta_{WLS} &= \tilde \Phi^+ \tilde y \qquad 
	\text{ mit } \tilde \Phi = W^{\frac{1}{2}} \Phi \text{ und } \tilde y = W^{\frac{1}{2}} y & \\
	&= \underset{\theta \in \mathbb{R}}{\argmin} \, f_{WLS}(\theta) = { (\Phi^T W \Phi)}^{-1} \Phi^T Wy
\end{flalign*}
\end{tcolorbox}


\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=\textbf{Ill-Posed Least Squares}]	
\textbf{Singular Value Decomposition: } $A = USV^T \in\mathbb R^{mxn} $
\begin{addmargin}[1em]{0em}
	with $U\in\mathbb R^{mxm}$, $V\in\mathbb R^{nxn}$ and $S\in\mathbb R^{mxn}$ where $S$ is a diagonal Matrix with non-negative elements $(\sigma_1,\dots,\sigma_r, 0,\dots,0)$\\
\end{addmargin}
\textbf{Moore Penrose Pseudo Inverse: }
\begin{align*}
	\Phi^+ = VS^+ U^T = V(S^T S+\alpha I)^{-1} S^T U^T
\end{align*}
\hspace{1em} $\Phi^+$ therefore selects $\theta^* \in S^*$ with minimal norm \\
\textbf{Regularization for Least Squares:}
\begin{align*}
	\underset{a \rightarrow 0}{\lim} (\Phi^T \Phi + \alpha I)^{-1} \Phi^T = \Phi^+ \quad \text{with }\Phi^+ \, MPPI 
\end{align*}
\begin{align*}
\theta^* = (\Phi^T \Phi + \alpha \mathbb{I})^{-1} \Phi^T y
\end{align*}
\end{tcolorbox}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=\textbf{Statistical Analysis of WLS}]
\textbf{Expectation of Least Squares Estimator: }
\begin{align*}
	E \{\hat \theta_{WLS} \} = E \{ {(\Phi_{N}^{T} W \Phi_N)}^{-1} \Phi_{N}^{T} W y_N \} = \theta_0
\end{align*}
\textbf{Covariance of the least squares estimator: }
\begin{align*}
	\cov(\hat \theta_{WLS}) &= {(\Phi_{N}^{T} W \Phi_N)}^{-1} = {(\Phi_{N}^{T} \Sigma_{\in N }^{-1} \Phi_{N}) }^{-1} \\
	\cov(\hat \theta_{WLS}) &\succeq {(\Phi_{N}^{T} W \Phi_{N})}^{-1}
\end{align*}
\end{tcolorbox}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=\textbf{Example LLS}]
\textbf{Example of the Linear Least Square Estimator for: $N=2$}
\begin{equation*}
	\varepsilon(1) \sim \mathcal{N}(0|{\sigma}^{2}_{1}) \hfil \varepsilon(2) \sim 	\mathcal{N}(0|\sigma^{2}_{2})
\end{equation*}

\begin{flalign*}
	& N=2; \quad \Sigma_{\varepsilon_{N}} = 
	\begin{bmatrix} \sigma_{1}^{2} & 0 \\ 0 & \sigma_{2}^{2} \end{bmatrix} 
	\qquad
	W^{OPT} = \Sigma_{\varepsilon_N}^{-1} = 
	\begin{bmatrix} \frac {1}{\sigma_{1}^{2}}  & 0 \\ 0 & \frac {1}{\sigma_{2}^{2}}  \end{bmatrix} &
\end{flalign*}

\begin{flalign*}
	\cov(\hat \theta_{WLS}) &= {(Y_N - \Phi_N \theta)}^T \cdot W \cdot (Y_N - \Phi_N \theta ) & \\
	&= \sum_{k=1}^{2} (y(k) - \phi(k)^T \theta ) \cdot \frac {1}{\sigma_{k}^{2}} \cdot (y(k) - \phi(k)^T \theta) &
\end{flalign*}

\textbf{Measuring the goodness of Fit using:  ${R}^{2}$} \quad ($0\le {R}^{2} \le1$)
\begin{align*}
R^2 &= 1 - \frac{ { \lVert y_N - \Phi_N \hat \theta \rVert}_{2}^{2} }{ {\lVert y_N \rVert}_{2}^{2} } = 1 - \frac{ {\lVert \epsilon_N \rVert}_{2}^{2} }{ {\lVert y_N \rVert}_{2}^{2} } \\
&= \frac{ {\lVert y_N \rVert}_{2}^{2} - {\lVert \epsilon_N \rVert}_{2}^{2} }{ {\lVert y_N \rVert}_{2}^{2} } = \frac{ {\lVert \hat y_N \rVert}_{2}^{2} }{ {\lVert y_N \rVert}_{2}^{2} }
\end{align*}
Residual: $ \epsilon_N \uparrow \rightarrow \quad R^{2} \rightarrow 0 \,\,(\Rightarrow bad)$
\tcblower
\textbf{Estimating the Covariance with the Single Experiment: }
\begin{flalign*}
\hat \sigma_{\varepsilon}^{2} &:= \frac{1}{N-d} \sum_{k=1}^{N} (y(k)-\phi(k)^T \hat \theta_{LS})^2 = \frac{ {\lVert y_N - \phi_N \hat \theta_{LS} \rVert}_{2}^{2} }{ N-d } &\\
\hat \Sigma_{\hat \theta} &:= \hat \sigma_{\varepsilon}^{2} (\phi^{T}_{N} \phi_{N})^{-1} = \sigma_{\epsilon}^2(\Phi_N^+ \Phi_N^{+^T})=\frac{ {\lVert y_N -\phi_N \hat \theta_{LS} \rVert}_{2}^{2} }{ N-d } \cdot (\phi^{T}_{N} \phi_{N})^{-1} &
\end{flalign*}
\end{tcolorbox}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../HelpSheet"
%%% End:
